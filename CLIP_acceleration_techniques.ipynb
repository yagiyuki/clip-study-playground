{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0XNcK6pkZi5maZtoZbZ6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagiyuki/clip-study-playground/blob/main/CLIP_acceleration_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIPで大量に画像分類するときの高速化実装の工夫"
      ],
      "metadata": {
        "id": "8nlJuzTc6fmj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1hv216fAWq5O",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# 画像を管理するパスを作成\n",
        "mkdir -p data\n",
        "\n",
        "# dataディレクトリにsample.jpegと言う名前で画像ファイルを上げる\n",
        "\n",
        "# 画像を999枚複製（合計1000枚）\n",
        "for i in {1..999}; do\n",
        "  cp data/sample.jpeg data/sample_${i}.jpeg\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 特に工夫なしの場合"
      ],
      "metadata": {
        "id": "qfLTgBHX6t-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "import glob\n",
        "\n",
        "# デバイス設定（そのままFP32, コンパイルなし）\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "\n",
        "# トークナイザ／プロセッサ／モデルの読み込み（標準設定）\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "\n",
        "# 画像ファイルのリストを取得\n",
        "image_list = glob.glob('data/*')\n",
        "\n",
        "# 各画像ごとに（バッチもAMPも事前計算もなしで）処理\n",
        "for image_path in image_list:\n",
        "    # テキストをその都度トークナイズ\n",
        "    text_inputs = tokenizer(\n",
        "        [\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]\n",
        "    ).to(device)\n",
        "\n",
        "    # 画像を読み込んで前処理\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=[image], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # テキスト特徴量を毎回計算\n",
        "        text_features = model.get_text_features(**text_inputs)  # FP32\n",
        "\n",
        "        # 画像特徴量を毎回計算\n",
        "        image_features = model.get_image_features(inputs.pixel_values)  # FP32\n",
        "\n",
        "        # 類似度計算・確率化\n",
        "        text_probs = (image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # 結果表示\n",
        "    #print(f'Label probs of {image_path}:', text_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is7rMLBS6SUE",
        "outputId": "a343370a-b075-4127-8a71-ca080031d12e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.1 s, sys: 247 ms, total: 48.3 s\n",
            "Wall time: 49.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##【工夫ポイント1】torch.compileによるモデル最適化"
      ],
      "metadata": {
        "id": "lzoJ4ufK_tnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "\n",
        "# デバイス設定\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "\n",
        "# モデル読み込み＋コンパイル\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "model = torch.compile(model)  # ← ここだけが追加\n",
        "\n",
        "# 単純な推論ループ\n",
        "with torch.no_grad():\n",
        "    text = tokenizer([\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]).to(device)\n",
        "    for path in glob.glob('data/*'):\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # 毎回テキスト特徴量を計算\n",
        "        text_feats = model.get_text_features(**text)\n",
        "        img_feats  = model.get_image_features(inputs.pixel_values)\n",
        "        probs = (img_feats @ text_feats.T).softmax(dim=-1)\n",
        "        #print(path, probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYSOqyRf_sC3",
        "outputId": "97988883-4bbd-4e53-d5ee-e58c8a15e7d7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47.3 s, sys: 264 ms, total: 47.6 s\n",
            "Wall time: 48.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##【工夫ポイント2】テキスト特徴量の事前計算（ループ外で一度だけ）"
      ],
      "metadata": {
        "id": "J4z8QsYz_2s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "\n",
        "# デバイス設定\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "\n",
        "# モデル読み込み＋コンパイル\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "model = torch.compile(model)  # ← ここだけが追加\n",
        "\n",
        "# 単純な推論ループ\n",
        "with torch.no_grad():\n",
        "    text = tokenizer([\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]).to(device)\n",
        "    for path in glob.glob('data/*'):\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # 毎回テキスト特徴量を計算\n",
        "        text_feats = model.get_text_features(**text)\n",
        "        img_feats  = model.get_image_features(inputs.pixel_values)\n",
        "        probs = (img_feats @ text_feats.T).softmax(dim=-1)\n",
        "        #print(path, probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saj7I8Sk_8A_",
        "outputId": "7ab84c21-5255-4ba9-cc88-4dc059fd0783"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47.4 s, sys: 245 ms, total: 47.7 s\n",
            "Wall time: 49 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##【工夫ポイント3】torch.cuda.amp.autocastによる混合精度推論"
      ],
      "metadata": {
        "id": "32NtEuJW_25E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text = tokenizer([\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]).to(device)\n",
        "    text_feats = model.get_text_features(**text).half()\n",
        "\n",
        "    for path in glob.glob('data/*'):\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # ここだけが追加：autocastによるFP16混合精度\n",
        "        with torch.cuda.amp.autocast():\n",
        "            img_feats = model.get_image_features(inputs.pixel_values)\n",
        "\n",
        "        probs = (img_feats @ text_feats.T).softmax(dim=-1)\n",
        "        #print(path, probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad4VojnBADre",
        "outputId": "0db906b5-8a1a-4109-9de2-d33b5a639098"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 42.3 s, sys: 253 ms, total: 42.5 s\n",
            "Wall time: 43.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 【工夫ポイント4】バッチ処理による転送・呼び出し回数の削減"
      ],
      "metadata": {
        "id": "hzPiubgz_3vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "batch_size = 50\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text = tokenizer([\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]).to(device)\n",
        "    text_feats = model.get_text_features(**text)\n",
        "\n",
        "    image_paths = glob.glob('data/*')\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        # ここだけが追加：複数画像を一度に処理\n",
        "        batch = image_paths[i:i+batch_size]\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in batch]\n",
        "        inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        img_feats = model.get_image_features(inputs.pixel_values)\n",
        "\n",
        "        probs = (img_feats @ text_feats.T).softmax(dim=-1)\n",
        "        #for path, p in zip(batch, probs):\n",
        "        #    print(path, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrYnjbdFARkz",
        "outputId": "d1317e86-fe64-4b81-c470-96169cb99546"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.4 s, sys: 149 ms, total: 20.6 s\n",
            "Wall time: 21.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4つの工夫をした場合"
      ],
      "metadata": {
        "id": "nKqkpPGL6zm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel, AutoTokenizer\n",
        "import glob\n",
        "\n",
        "# デバイス設定\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "HF_MODEL_PATH = 'line-corporation/clip-japanese-base'\n",
        "\n",
        "# トークナイザ／プロセッサ／モデルの読み込み\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "processor = AutoImageProcessor.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True).to(device)\n",
        "\n",
        "# ─────── 高速化の工夫ポイント1 ───────\n",
        "# torch.compile でモデルをグラフ化＆最適化し、繰り返し推論のオーバーヘッドを削減\n",
        "model = torch.compile(model)\n",
        "\n",
        "# テキストをトークン化（ループ外で一度だけ行う）\n",
        "text = tokenizer(\n",
        "    [\"ベンチプレス\", \"スクワット\", \"デッドリフト\"]\n",
        ").to(device)\n",
        "\n",
        "# 画像ファイルのリストを取得\n",
        "image_list = glob.glob('data/*')\n",
        "batch_size = 50\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ─────── 高速化の工夫ポイント2 ───────\n",
        "    # テキスト特徴量をループ前に事前計算し、毎バッチ再計算を避ける\n",
        "    text_features = model.get_text_features(**text)\n",
        "    # ─────── 型合わせ対策 ───────\n",
        "    # image_features が FP16 なので、text_features も FP16 にキャスト\n",
        "    text_features = text_features.half()\n",
        "\n",
        "    for i in range(0, len(image_list), batch_size):\n",
        "        batch_image_list = image_list[i:i+batch_size]\n",
        "\n",
        "        # ─────── 高速化の工夫ポイント4 ───────\n",
        "        # バッチ処理により、CPU⇔GPU転送回数とモデル呼び出し回数を削減\n",
        "        images = [Image.open(p).convert(\"RGB\") for p in batch_image_list]\n",
        "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # ─────── 高速化の工夫ポイント3 ───────\n",
        "        # torch.cuda.amp.autocast() を使い、FP16混合精度で演算スループットを向上\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_features = model.get_image_features(inputs.pixel_values)\n",
        "\n",
        "        # FP16 同士なので内積計算がエラーなく実行可能\n",
        "        text_probs = (image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        # 各画像のラベル確率を表示\n",
        "        #for image_path, probs in zip(batch_image_list, text_probs):\n",
        "        #    print(f'Label probs of {image_path}:', probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGeHRVSWxrZ",
        "outputId": "862c1e58-6e7d-4239-b60d-924facd21260"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19.9 s, sys: 159 ms, total: 20.1 s\n",
            "Wall time: 21 s\n"
          ]
        }
      ]
    }
  ]
}